{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于在训练时要将y变为(5000,10)的数据，故需要使用one-hot，独热编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10], dtype=uint8), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encode = OneHotEncoder(sparse=False)\n",
    "y_hot = encode.fit_transform(y)\n",
    "\n",
    "y[0], y_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播和损失函数\n",
    "cost = $J \\left(\\theta \\right) = - \\frac{1}{m} \\left[\\sum\\limits_{i=1}^m \\sum\\limits_{k=1}^K y_k^{\\left(i \\right)} log\\left( h_{\\theta}\\left(x^{\\left(i \\right)}\\right)\\right)_k+\\left(1-y_k^{\\left(i \\right)}\\right) log\\left(1-\\left(h_{\\theta}\\left(x^{\\left(i \\right)}\\right)\\right)_k \\right)\\right] + \\frac{\\lambda}{2m}\\sum\\limits_{l=1}^{L-1}\\sum\\limits_{i=1}^{S_l}\\sum\\limits_{j=1}^{S_{l+1}}\\left(\\theta_{ji}^\\left(l \\right) \\right)^2$ 此为加上正则项的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, theta1, theta2):\n",
    "    \n",
    "    a0 = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1)\n",
    "    z1 = np.dot(a0, theta1.T)\n",
    "    a1 = np.insert(sigmoid(z1), 0, values=np.ones(z1.shape[0]), axis=1)\n",
    "    z2 = np.dot(a1, theta2.T)\n",
    "    y_hat = sigmoid(z2)\n",
    "    \n",
    "    return a0, z1, a1, z2, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(params, input_size, hidden_size, num_labels, X, y, labmda):\n",
    "    # 由于使用了优化函数，所以theta1和theta2要合并并且变为一维数组\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #params还原theta1和theta2\n",
    "    theta1 = params[0:(input_size+1) * hidden_size].reshape(hidden_size, (input_size+1))\n",
    "    theta2 = params[(input_size+1) * hidden_size:].reshape(num_labels, (hidden_size+1))\n",
    "    \n",
    "    #前向传播\n",
    "    a0, z1, a1, z2, y_hat = forward(X, theta1, theta2)\n",
    "\n",
    "    # y_hat.shape = (m, 10) 注：这里不好使用向量化\n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "    \n",
    "        J += np.sum(np.multiply(-y[i, :], np.log(y_hat[i, :])) - np.multiply((1 - y[i, :]), np.log(1 - y_hat[i, :]))) #一维数组相同位置相乘直接用multiply，并且此种方式得到的是一个一维数组\n",
    "    \n",
    "    return J / m\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theta随机初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    input_size = 400\n",
    "    hidden_size = 25\n",
    "    num_labels = 10\n",
    "    labmda = 1.0\n",
    "    params = np.random.randn(hidden_size * (input_size+1) + num_labels * (hidden_size+1)) * 0.01        #随机初始化\n",
    "#     params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)) - 0.5) * 0.25\n",
    "\n",
    "    return params, input_size, hidden_size, num_labels, labmda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, input_size, hidden_size, num_labels, labmda = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.937138442774584"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(params, input_size, hidden_size, num_labels, X, y_hot, labmda) #输入的y为特征拓展后的y_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化代价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_cost(params, input_size, hidden_size, num_labels, X, y, labmda):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #params还原theta1和theta2\n",
    "    theta1 = params[0:(input_size+1) * hidden_size].reshape(hidden_size, (input_size+1))\n",
    "    theta2 = params[(input_size+1) * hidden_size:].reshape(num_labels, (hidden_size+1))\n",
    "    \n",
    "    #前向传播\n",
    "    a0, z1, a1, z2, y_hat = forward(X, theta1, theta2)\n",
    "\n",
    "    # y_hat.shape = (m, 10) 注：这里不好使用向量化\n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "    \n",
    "        J += np.sum(np.multiply(-y[i, :], np.log(y_hat[i, :])) - np.multiply((1 - y[i, :]), np.log(1 - y_hat[i, :])))\n",
    "    \n",
    "    reg_1 = labmda / (2 * m) * np.sum(np.power(theta1[:, 1:], 2))\n",
    "    reg_2 = labmda / (2 * m) * np.sum(np.power(theta2[:, 1:], 2)) #正则化不包括bias的那个theta\n",
    "    J = J / m + reg_1 + reg_2\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.9372425282588095"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularized_cost(params, input_size, hidden_size, num_labels, X, y_hot, labmda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return np.multiply(sigmoid(z), (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params, input_size, hidden_size, num_labels, X, y, labmda):\n",
    "    m = X.shape[0]\n",
    "    #params还原theta1和theta2\n",
    "    theta1 = params[0:(input_size+1) * hidden_size].reshape(hidden_size, (input_size+1))\n",
    "    theta2 = params[(input_size+1) * hidden_size:].reshape(num_labels, (hidden_size+1))\n",
    "    \n",
    "    #前向传播\n",
    "    a0, z1, a1, z2, y_hat = forward(X, theta1, theta2)\n",
    "    \n",
    "    #反向传播\n",
    "    dz2 = y_hat - y  #(m, 10)                                         #m, 10\n",
    "    dtheta2 = 1. / m * np.dot(dz2.T, a1)                              #10, 26\n",
    "    dz1 = np.multiply(np.dot(dz2, theta2), np.multiply(a1, 1 - a1))   #m, 26\n",
    "    dtheta1 = 1. / m * np.dot((dz1[:, 1:]).T, a0)                     #25, 401, dz1的第一列是bias的导数值，故舍去\n",
    "    \n",
    "    #正则化\n",
    "    dtheta1[:, 1:] = dtheta1[:, 1:] + labmda / m * theta1[:, 1:]\n",
    "    dtheta2[:, 1:] = dtheta2[:, 1:] + labmda / m * theta2[:, 1:]      #第j列不参与正则化\n",
    "    \n",
    "    grads = np.concatenate((np.ravel(dtheta1), np.ravel(dtheta2)))\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = backprop(params, input_size, hidden_size, num_labels, X, y_hot, labmda)\n",
    "grads.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码谨慎使用，我i5-4210的处理器算了好久"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "fmin = opt.fmin_tnc(func=regularized_cost, x0=params, fprime=backprop,\n",
    "                    args=(input_size, hidden_size, num_labels, X, y_hot, labmda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmin[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还是看的出来拟合效果还是不错的，但是文档里面给出的准确率是95.3%不知道是不是优化算法不同的原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 99.7%\n"
     ]
    }
   ],
   "source": [
    "params_2 = fmin[0]\n",
    "theta1 = params_2[0:(input_size+1) * hidden_size].reshape(hidden_size, (input_size+1))\n",
    "theta2 = params_2[(input_size+1) * hidden_size:].reshape(num_labels, (hidden_size+1))\n",
    "_, _, _, _, predict = forward(X, theta1, theta2)\n",
    "predict = predict.argmax(axis=1) + 1\n",
    "predict = predict.reshape(predict.shape[0], 1)\n",
    "predict = predict[predict == y]\n",
    "print(\"Accuracy is \" + str(len(predict) / len(y) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking 不带正则化的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果误差小于$10^{-9}$则说明执行正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(params, input_size, hidden_size, num_labels, X, y, epsilon=0.0001):\n",
    "    m = X.shape[0]\n",
    "    #params还原theta1和theta2\n",
    "    theta1 = params[0:(input_size+1) * hidden_size].reshape(hidden_size, (input_size+1))\n",
    "    theta2 = params[(input_size+1) * hidden_size:].reshape(num_labels, (hidden_size+1))\n",
    "    \n",
    "    #前向传播\n",
    "    a0, z1, a1, z2, y_hat = forward(X, theta1, theta2)\n",
    "    \n",
    "    #反向传播\n",
    "    dz2 = y_hat - y  #(m, 10)                                         #m, 10\n",
    "    dtheta2 = 1. / m * np.dot(dz2.T, a1)                              #10, 26\n",
    "#     dz1 = np.multiply(np.dot(dz2, theta2), np.multiply(a1, 1 - a1))   #m, 26\n",
    "#     dtheta1 = 1. / m * np.dot((dz1[:, 1:]).T, a0)                     #25, 401, dz1的第一列是bias的导数值，故舍去\n",
    "    \n",
    "    #上面是backprop里面的直接抄过来\n",
    "    #checking   此程序只检查theata2\n",
    "    check = np.zeros(theta2.shape)\n",
    "    for i in range(theta2.shape[0]):\n",
    "        for j in range(theta2.shape[1]):\n",
    "            paramsplus = params\n",
    "            paramsmins = params\n",
    "            paramsplus[i * theta2.shape[1] + j + theta1.shape[0] * theta1.shape[1]] += epsilon\n",
    "            paramsmins[i * theta2.shape[1] + j + theta1.shape[0] * theta1.shape[1]] -= epsilon\n",
    "            check[i, j] = (cost(paramsplus, input_size, hidden_size, num_labels, X, y, 1.0) - cost(paramsmins, input_size, hidden_size, num_labels, X, y, 1.0)) / (2 * epsilon)\n",
    "#             print(cost(paramsplus, input_size, hidden_size, num_labels, X, y, 1.0), cost(paramsmins, input_size, hidden_size, num_labels, X, y, 1.0))\n",
    "    \n",
    "    return check, dtheta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "check, dtheta2 = gradient_checking(params, input_size, hidden_size, num_labels, X, y_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是误差，感觉有点大了，但是还没检查出来是哪个步骤出错了,但是我将另外一个大神的代码改了改也拿来算梯度检测发现和我的误差是一样的，所以不知道是不是我梯度检测是方式错了。 后来发现epsilon小于100损失函数根本没有变化check全为0，所以有可能是上面计算check的地方错误了，待修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.power(check - dtheta2, 2)) / (np.sum(np.power(check, 2)) + np.sum(np.power(dtheta2, 2))) #计算梯度检测的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the hidden layer未做"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
